{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. List all previous experiments using the `smexperiments` SDK.\n",
    "2. Delete all `Experiments`, `Trials`, and `TrialComponents` from experiments run before today.\n",
    "3. Let's extend our scikit-learn training example from chapter 3 to incorporate experiment tracking:\n",
    "    1. Make a copy of `sklearn_rf.py` and name it `sklearn-experiment.py`. Update `sklearn-experiment.py` to accept 3 additional command line arguments: 1) **max_depth**: type=int, default=None, 2)**min_samples_split**: type=int, default=2, and 3)**validation**': type=str, `default=os.environ['SM_CHANNEL_VALIDATION']`. Also update the script to calculate the accuracy on the test set and print it out to screen in the format `Test Accuracy: <value>;`.\n",
    "    2. In a Jupyter notebook, create an `Experiment` object with `experiment_name` of `churn-experiment`.\n",
    "    3. For each `max_depth` in `[2, 4, 8, 10]` and `min_samples_split` in `[2, 5, 10, 20]`:\n",
    "        * create a `Trial` object and an `sagemaker.sklearn.estimator.SKLearn` object for training. \n",
    "        * Pass the appropriate hyperparameter values to the `hyperparameter` argument. \n",
    "        * Pass `metric_definitions=[{'Name':'test:accuracy', 'Regex':'Test Accuracy: (.*?)%;'}]`\n",
    "    4. Fit the estimator and pass in the appropriate experiment configuration and train/validation files. Set `wait=False` in order to run all of the training jobs in parallel.\n",
    "    5. When training completes, create an `ExperimentAnalytics` object to retrieve the results. Which combination of hyperparameters maximizes the metric on the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments import experiment\n",
    "import smexperiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in experiment.Experiment.list():\n",
    "    print(exp.experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_sum in experiment.Experiment.list(created_before=datetime.datetime(2020, 8, 29)):\n",
    "    exp = experiment.Experiment.load(experiment_name=exp_sum.experiment_name)\n",
    "    exp.delete_all(action='--force')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /root/sagemaker-course/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../scripts/sklearn/sklearn_rf.py\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters are described here. In this simple example we are just including one hyperparameter.\n",
    "    parser.add_argument('--n_estimators', type=int, default=100)\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Take the set of files and read them all into a single pandas dataframe\n",
    "    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "    raw_data = [ pd.read_csv(file, header=None, engine=\"python\") for file in input_files ]\n",
    "    train_data = pd.concat(raw_data)\n",
    "\n",
    "    # labels are in the first column\n",
    "    train_y = train_data.ix[:,0]\n",
    "    train_X = train_data.ix[:,1:]\n",
    "\n",
    "    # Here we support a single hyperparameter, 'n_estimators'. Note that you can add as many\n",
    "    # as your training my require in the ArgumentParser above.\n",
    "    n_estimators = args.n_estimators\n",
    "\n",
    "    # Now use scikit-learn's random forest classifier to train the model.\n",
    "    clf = ensemble.RandomForestClassifier(n_estimators=n_estimators)\n",
    "    clf = clf.fit(train_X, train_y)\n",
    "\n",
    "    # Serialize the model.\n",
    "    joblib.dump(clf, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialize and return fitted model\n",
    "    \n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../scripts/sklearn/sklearn-experiment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../scripts/sklearn/sklearn-experiment.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters are described here. In this simple example we are just including one hyperparameter.\n",
    "    parser.add_argument('--n_estimators', type=int, default=100)\n",
    "    parser.add_argument('--max_depth', type=int, default=None)\n",
    "    parser.add_argument('--min_samples_split', type=int, default=2)\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--validation', type=str, default=os.environ['SM_CHANNEL_VALIDATION'])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Take the set of files and read them all into a single pandas dataframe\n",
    "    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "    raw_data = [ pd.read_csv(file, header=None, engine=\"python\") for file in input_files ]\n",
    "    train_data = pd.concat(raw_data)\n",
    "\n",
    "    # labels are in the first column\n",
    "    train_y = train_data.ix[:,0]\n",
    "    train_X = train_data.ix[:,1:]\n",
    "    \n",
    "    input_files = [ os.path.join(args.validation, file) for file in os.listdir(args.validation) ]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "    raw_data = [ pd.read_csv(file, header=None, engine=\"python\") for file in input_files ]\n",
    "    validation_data = pd.concat(raw_data)\n",
    "    \n",
    "    # labels are in the first column\n",
    "    validation_y = validation_data.ix[:,0]\n",
    "    validation_X = validation_data.ix[:,1:]\n",
    "\n",
    "    # Here we support a single hyperparameter, 'n_estimators'. Note that you can add as many\n",
    "    # as your training my require in the ArgumentParser above.\n",
    "    n_estimators = args.n_estimators\n",
    "\n",
    "    # Now use scikit-learn's random forest classifier to train the model.\n",
    "    clf = ensemble.RandomForestClassifier(n_estimators=n_estimators)\n",
    "    clf = clf.fit(train_X, train_y)\n",
    "    \n",
    "    y_pred = clf.predict(validation_X)\n",
    "    \n",
    "    test_accuracy = accuracy_score(validation_y, y_pred)\n",
    "    \n",
    "    print(f'Test Accuracy: {test_accuracy};')\n",
    "\n",
    "    # Serialize the model.\n",
    "    joblib.dump(clf, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialize and return fitted model\n",
    "    \n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts will be written to s3://sagemaker-course-20200812/churn\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# S3 bucket information\n",
    "BUCKET = 'sagemaker-course-20200812'\n",
    "PREFIX = 'churn'\n",
    "LOCAL_DATA_DIRECTORY = f'../data/{PREFIX}'\n",
    "print(f\"Artifacts will be written to s3://{BUCKET}/{PREFIX}\")\n",
    "\n",
    "# Session variables we'll use throughout the notebook\n",
    "sagemaker_session = sagemaker.Session()\n",
    "boto_session = sagemaker_session.boto_session\n",
    "sagemaker_client = boto_session.client('sagemaker')\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = experiment.Experiment.create(experiment_name='churn-experiment', \n",
    "                                   description='Experiment for churn dataset', \n",
    "                                   sagemaker_boto_client=sagemaker_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from smexperiments.trial import Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker:Creating training-job with name: sklearn-experiment-2020-08-29-15-46-08-392\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker:Creating training-job with name: sklearn-experiment-2020-08-29-15-46-08-744\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker:Creating training-job with name: sklearn-experiment-2020-08-29-15-46-11-734\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker:Creating training-job with name: sklearn-experiment-2020-08-29-15-46-12-198\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker:Creating training-job with name: sklearn-experiment-2020-08-29-15-46-17-929\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker:Creating training-job with name: sklearn-experiment-2020-08-29-15-46-18-759\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker:Creating training-job with name: sklearn-experiment-2020-08-29-15-46-19-670\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker:Creating training-job with name: sklearn-experiment-2020-08-29-15-46-21-579\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker:Creating training-job with name: sklearn-experiment-2020-08-29-15-46-22-111\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker:Creating training-job with name: sklearn-experiment-2020-08-29-15-46-27-433\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker:Creating training-job with name: sklearn-experiment-2020-08-29-15-46-27-928\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker:Creating training-job with name: sklearn-experiment-2020-08-29-15-46-28-564\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker:Creating training-job with name: sklearn-experiment-2020-08-29-15-46-29-129\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker:Creating training-job with name: sklearn-experiment-2020-08-29-15-46-31-874\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker:Creating training-job with name: sklearn-experiment-2020-08-29-15-46-36-116\n",
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker:Creating training-job with name: sklearn-experiment-2020-08-29-15-46-37-794\n"
     ]
    }
   ],
   "source": [
    "s3_input_train = sagemaker.TrainingInput(s3_data='s3://sagemaker-course-20200812/churn/train.csv',\n",
    "                                         content_type='csv')\n",
    "s3_input_validation = sagemaker.TrainingInput(s3_data='s3://sagemaker-course-20200812/churn/validation.csv',\n",
    "                                              content_type='csv')\n",
    "\n",
    "for max_depth in [2, 4, 8, 10]:\n",
    "    for min_samples_split in [2, 5, 10, 20]:\n",
    "\n",
    "        trial_name = f\"sklearn-rf-{max_depth}-max-depth-{min_samples_split}-min-samples-split\"\n",
    "        sklearn_trial = Trial.create(\n",
    "            trial_name=trial_name, \n",
    "            experiment_name=exp.experiment_name,\n",
    "            sagemaker_boto_client=sagemaker_client,\n",
    "        )\n",
    "        \n",
    "        sklearn_estimator = SKLearn(\n",
    "            framework_version='0.20.0',\n",
    "            py_version='py3',\n",
    "            entry_point='../scripts/sklearn/sklearn-experiment.py',\n",
    "            code_location=f's3://{BUCKET}/{PREFIX}',\n",
    "            hyperparameters={'n_estimators': 50,\n",
    "                             'max_depth': max_depth,\n",
    "                             'min_samples_split': min_samples_split},\n",
    "            role=role,\n",
    "            instance_type='ml.c4.xlarge',\n",
    "            output_path=f's3://{BUCKET}/{PREFIX}',\n",
    "            base_job_name='sklearn-experiment',\n",
    "            sagemaker_session=sagemaker_session,\n",
    "            metric_definitions=[\n",
    "                {'Name':'test:accuracy', 'Regex':'Test Accuracy: (.*?);'}]\n",
    "        )\n",
    "        \n",
    "        sklearn_estimator.fit({'train': s3_input_train, 'validation': s3_input_validation},\n",
    "                              experiment_config={\n",
    "                                  'TrialName': sklearn_trial.trial_name,\n",
    "                                  'TrialComponentDisplayName': 'Training'\n",
    "                              },\n",
    "                              wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics = ExperimentAnalytics(experiment_name=exp.experiment_name,\n",
    "                                sagemaker_session=sagemaker_session)\n",
    "df = analytics.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TrialComponentName', 'DisplayName', 'SourceArn', 'SageMaker.ImageUri',\n",
       "       'SageMaker.InstanceCount', 'SageMaker.InstanceType',\n",
       "       'SageMaker.VolumeSizeInGB', 'max_depth', 'min_samples_split',\n",
       "       'n_estimators', 'sagemaker_container_log_level', 'sagemaker_job_name',\n",
       "       'sagemaker_program', 'sagemaker_region', 'sagemaker_submit_directory',\n",
       "       'test:accuracy - Min', 'test:accuracy - Max', 'test:accuracy - Avg',\n",
       "       'test:accuracy - StdDev', 'test:accuracy - Last',\n",
       "       'test:accuracy - Count', 'train - MediaType', 'train - Value',\n",
       "       'validation - MediaType', 'validation - Value',\n",
       "       'SageMaker.DebugHookOutput - MediaType',\n",
       "       'SageMaker.DebugHookOutput - Value',\n",
       "       'SageMaker.ModelArtifact - MediaType',\n",
       "       'SageMaker.ModelArtifact - Value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
